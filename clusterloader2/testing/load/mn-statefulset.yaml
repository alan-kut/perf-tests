{{$Image := DefaultParam .Image "registry.k8s.io/pause:3.9"}}
{{$RUN_ON_ARM_NODES := DefaultParam .CL2_RUN_ON_ARM_NODES false}}

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: {{.Name}}
  labels:
    group: aiml
    multinetwork{{.Index}}: "true"
spec:
  podManagementPolicy: Parallel
  replicas: {{RandIntRange .ReplicasMin .ReplicasMax}}
  selector:
    matchLabels:
      name: {{.Name}}
  template:
    metadata:
      labels:
        group: aiml
        name: {{.Name}}
        multinetwork{{.Index}}: "true"
      annotations:
        networking.gke.io/default-interface: 'eth0'
        networking.gke.io/interfaces: |
          [
            {
              "interfaceName": "eth0",
              "network": "default"
            },
            {
              "interfaceName": "eth1",
              "network": "net{{.Index}}-sub{{.Index}}-sec0"
            },
            {
              "interfaceName": "eth2",
              "network": "net{{.Index}}-sub{{.Index}}-sec1"
            },
            {
              "interfaceName": "eth3",
              "network": "net{{.Index}}-sub{{.Index}}-sec2"
            },
            {
              "interfaceName": "eth4",
              "network": "net{{.Index}}-sub{{.Index}}-sec3"
            }
          ]
    spec:
      schedulerName: gke.io/high-throughput-scheduler
      nodeSelector:
        multinetwork{{.Index}}: "true"
      containers:
      - name: {{.Name}}
        image: {{$Image}}
        ports:
        - containerPort: 99
          hostPort: 7300
          protocol: TCP
        resources:
          # Keep the CpuRequest/MemoryRequest request equal percentage of 1-core, 4GB node.
          # For now we're setting it to 0.5%.
          requests:
            cpu: 5m
            memory: "20M"
      terminationGracePeriodSeconds: 1
      tolerations:
      {{if $RUN_ON_ARM_NODES}}
      - key: "kubernetes.io/arch"
        operator: Equal
        value: arm64
        effect: NoSchedule
      {{end}}
  serviceName: mn-{{.Index}}
